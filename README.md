# Bootcamp_Project_6

#Problem statement
Discussing things you care about can be difficult. Many people stop expressing their minds online due to the fear of abuse and harassment, and give up on seeking different opinions. Platforms struggle to facilitate conversations effectively, which forces many communities to restrict or disable user comments. In this context, the majority of platforms and parties have developed methods to identify and categorise different types of talks according to their level of toxicity. The study of harmful online behaviours, such as toxic remarks, is one area of emphasis (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). Different models have so far been developed to manage the degree of toxicity in talks. The existing models, however, continue to have flaws and don't let consumers choose the forms of toxicity in which they're most interested in finding(e.g. some platforms may be fine with profanity, but not with other types of toxic content).

